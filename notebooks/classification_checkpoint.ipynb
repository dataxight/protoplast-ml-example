{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66267ca4-71ee-44a2-b428-723c23e02526",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Showcasing Protoplast Checkpointing in Cell-line Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook showcases the checkpointing feature in PROTOplast, which enables resuming model training after finishing one dataset & switching to another. It demonstrates how to save and load training checkpoints, making it easy to continue model development without starting from scratch. This is particularly useful for long training sessions, experimentation with various datasets, or training across multiple sessions or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vblA",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - Logging initialized. Current level is: INFO\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import protoplast as pt\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from anndata.experimental import AnnCollection\n",
    "from protoplast.scrna.anndata.lightning_models import LinearClassifier\n",
    "from protoplast.scrna.anndata.trainer import RayTrainRunner\n",
    "from protoplast.scrna.anndata.torch_dataloader import DistributedAnnDataset\n",
    "from protoplast.scrna.anndata.torch_dataloader import cell_line_metadata_cb, DistributedCellLineAnnDataset\n",
    "\n",
    "from ray.train import Checkpoint\n",
    "from ray.train.lightning import RayDDPStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95fe1cd-efc8-4199-b83d-8615ce1efdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(version(\"protoplast\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Dataset pre-processing\n",
    "\n",
    "We begin by reading the two datasets used to train the cell-line classification model in this notebook. To ensure compatibility, the model requires that both datasets have the same output dimensions\n",
    "\n",
    "In the following section, we create a unified view by performing an **inner join** on the two datasets based on shared features. During this step, we:\n",
    "\n",
    "- Identify and record the **number of output classes** (cell-lines),\n",
    "- Extract the list of **cell-line** of both dataset.\n",
    "\n",
    "This alignment is essential to ensure the model receives a consistent input/output structure regardless of the dataset source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATHS = [\"/mnt/hdd2/tan/tahoe100m/plate1_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\",\n",
    "           \"/mnt/hdd2/tan/tahoe100m/plate2_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\"]\n",
    "adatas = [anndata.io.read_h5ad(p, backed = \"r\") for p in DS_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view of all dataset\n",
    "collection = AnnCollection(adatas, join_vars = \"inner\")\n",
    "\n",
    "# Record the cell-lines (output classes) in both datasets\n",
    "cell_lines = collection.obs.cell_line.unique().tolist()\n",
    "cell_lines_count = collection.obs.cell_line.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 3. Configure training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_per_worker = 12\n",
    "test_size = 0.0 # We don't have the test step in the model, so we can set this to 0\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 4. Train on `plate1_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate1_adata = adatas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Hstk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>gene_count</th>\n",
       "      <th>tscp_count</th>\n",
       "      <th>mread_count</th>\n",
       "      <th>drugname_drugconc</th>\n",
       "      <th>drug</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>sublibrary</th>\n",
       "      <th>BARCODE</th>\n",
       "      <th>pcnt_mito</th>\n",
       "      <th>S_score</th>\n",
       "      <th>G2M_score</th>\n",
       "      <th>phase</th>\n",
       "      <th>pass_filter</th>\n",
       "      <th>cell_name</th>\n",
       "      <th>plate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARCODE_SUB_LIB_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_001_025-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1676</td>\n",
       "      <td>2441</td>\n",
       "      <td>2892</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0131</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_025</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>-0.095055</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>A-172</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_026-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1657</td>\n",
       "      <td>2454</td>\n",
       "      <td>2925</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0480</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_026</td>\n",
       "      <td>0.042787</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.650549</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>PANC-1</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_048-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1749</td>\n",
       "      <td>2521</td>\n",
       "      <td>2963</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0293</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_048</td>\n",
       "      <td>0.056724</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.308791</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>HEC-1-A</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_076-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>834</td>\n",
       "      <td>1038</td>\n",
       "      <td>1258</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0397</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_076</td>\n",
       "      <td>0.066474</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.245788</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>LS 180</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_088-lib_841</th>\n",
       "      <td>smp_1495</td>\n",
       "      <td>1275</td>\n",
       "      <td>1710</td>\n",
       "      <td>2006</td>\n",
       "      <td>[('Infigratinib', 0.05, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1097</td>\n",
       "      <td>lib_841</td>\n",
       "      <td>01_001_088</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.085348</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>C32</td>\n",
       "      <td>plate1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sample  gene_count  tscp_count  mread_count  \\\n",
       "BARCODE_SUB_LIB_ID                                                  \n",
       "01_001_025-lib_841  smp_1495        1676        2441         2892   \n",
       "01_001_026-lib_841  smp_1495        1657        2454         2925   \n",
       "01_001_048-lib_841  smp_1495        1749        2521         2963   \n",
       "01_001_076-lib_841  smp_1495         834        1038         1258   \n",
       "01_001_088-lib_841  smp_1495        1275        1710         2006   \n",
       "\n",
       "                                 drugname_drugconc          drug  cell_line  \\\n",
       "BARCODE_SUB_LIB_ID                                                            \n",
       "01_001_025-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0131   \n",
       "01_001_026-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0480   \n",
       "01_001_048-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0293   \n",
       "01_001_076-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_0397   \n",
       "01_001_088-lib_841  [('Infigratinib', 0.05, 'uM')]  Infigratinib  CVCL_1097   \n",
       "\n",
       "                   sublibrary     BARCODE  pcnt_mito   S_score  G2M_score  \\\n",
       "BARCODE_SUB_LIB_ID                                                          \n",
       "01_001_025-lib_841    lib_841  01_001_025   0.025399 -0.066667  -0.095055   \n",
       "01_001_026-lib_841    lib_841  01_001_026   0.042787  0.128571   0.650549   \n",
       "01_001_048-lib_841    lib_841  01_001_048   0.056724  0.242857   0.308791   \n",
       "01_001_076-lib_841    lib_841  01_001_076   0.066474  0.009524   0.245788   \n",
       "01_001_088-lib_841    lib_841  01_001_088   0.028655 -0.100000  -0.085348   \n",
       "\n",
       "                   phase pass_filter cell_name   plate  \n",
       "BARCODE_SUB_LIB_ID                                      \n",
       "01_001_025-lib_841    G1        full     A-172  plate1  \n",
       "01_001_026-lib_841   G2M        full    PANC-1  plate1  \n",
       "01_001_048-lib_841   G2M        full   HEC-1-A  plate1  \n",
       "01_001_076-lib_841   G2M        full    LS 180  plate1  \n",
       "01_001_088-lib_841    G1        full       C32  plate1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate1_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ROlb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 05:59:05,041\tINFO worker.py:2003 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-07 05:59:05,068\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example/notebooks'.\n",
      "2025-11-07 05:59:05,089\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_540b6eeb2de965aa.zip' (2.08MiB) to Ray cluster...\n",
      "2025-11-07 05:59:05,107\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_540b6eeb2de965aa.zip'.\n",
      "/mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "trainer = RayTrainRunner(\n",
    "    LinearClassifier,\n",
    "    DistributedCellLineAnnDataset,\n",
    "    model_keys = [\"num_genes\",\n",
    "                  \"num_classes\"],\n",
    "    metadata_cb = cell_line_metadata_cb,\n",
    "    sparse_key = \"X\",\n",
    "    runtime_env_config = {\"working_dir\": os.getcwd()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b7c7b1-7623-487c-af34-b1954af570a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protoplast.scrna.anndata.trainer - INFO - Using 1 workers where each worker uses: {'CPU': 12, 'GPU': 1}\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of val_split: 66 length of test_split: 0, length of train_split: 268\n",
      "protoplast.scrna.anndata.strategy - INFO - Dropping 4 mini-batches\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of after dropping remainder val_split: 66, length of test_split: 0, length of train_split: 268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=2964641)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainController pid=2964641)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=2964641)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=2964641)\u001b[0m Attempting to start training worker group of size 1 with the following resources: [{'CPU': 12, 'GPU': 1}] * 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=2964641)\u001b[0m Started training worker group of size 1: \n",
      "\u001b[36m(TrainController pid=2964641)\u001b[0m - (ip=192.168.1.226, pid=2965202) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python3 /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/pyth ...\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return _C._get_float32_matmul_precision()\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 0 | model   | Linear           | 3.1 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 3.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 3.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 12.542    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 2         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   warnings.warn(  # warn only once\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:473: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_compressed_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.92it/s]\n",
      "                                                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m   return torch.sparse_csr_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/4284 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 1/4284 [00:13<16:22:35,  0.07it/s, v_num=0, train_loss=3.980]\n",
      "Epoch 0:   0%|          | 2/4284 [00:13<8:12:29,  0.14it/s, v_num=0, train_loss=3.840] \n",
      "...\n",
      "...\n",
      "...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4278/4284 [01:38<00:00, 43.47it/s, v_num=0, train_loss=0.137] \n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4279/4284 [01:38<00:00, 43.47it/s, v_num=0, train_loss=0.211]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [01:38<00:00, 43.50it/s, v_num=0, train_loss=0.145] \n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1056 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/1056 [00:00<00:04, 220.40it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 2/1056 [00:00<00:10, 99.65it/s] \u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m \n",
     "...\n",
     "...\n",
     "...\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1052/1056 [00:20<00:00, 50.44it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1053/1056 [00:20<00:00, 50.45it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1054/1056 [00:20<00:00, 50.46it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1055/1056 [00:20<00:00, 50.48it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1056/1056 [00:20<00:00, 50.49it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [02:13<00:00, 32.19it/s, v_num=0, train_loss=0.145]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [02:13<00:00, 32.15it/s, v_num=0, train_loss=0.145]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [02:13<00:00, 32.12it/s, v_num=0, train_loss=0.145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/training_results/ray_train_run-2025-11-07_05-59-18/checkpoint_2025-11-07_06-02-11.615409)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/home/dtran/training_results/ray_train_run-2025-11-07_05-59-18/checkpoint_2025-11-07_06-02-11.615409), metrics={'train_loss': 0.1452408730983734, 'val_acc': 0.982025146484375, 'epoch': 0, 'step': 4284}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=2965202)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# Start training process. The output of the training phase will be output to the cell above where \n",
    "# we initialize a ray train runner.\n",
    "result = trainer.train([DS_PATHS[0]],\n",
    "                       batch_size = 1024,\n",
    "                       test_size = test_size, \n",
    "                       val_size = val_size,\n",
    "                       num_workers = 1,\n",
    "                       thread_per_worker = thread_per_worker,\n",
    "                       result_storage_path = \"~/training_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8838590-b2f2-47b1-b3df-17fc54312704",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 5. Train on `plate2_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab` dataset\n",
    "\n",
    "We now have a checkpoint saved after training the classification model using the first dataset. We need to pass into `train()` the path to the checkpoint file. This path can be retrieved from the result trainer in previous `train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate2_adata = adatas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "DnEU",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>gene_count</th>\n",
       "      <th>tscp_count</th>\n",
       "      <th>mread_count</th>\n",
       "      <th>drugname_drugconc</th>\n",
       "      <th>drug</th>\n",
       "      <th>cell_line</th>\n",
       "      <th>sublibrary</th>\n",
       "      <th>BARCODE</th>\n",
       "      <th>pcnt_mito</th>\n",
       "      <th>S_score</th>\n",
       "      <th>G2M_score</th>\n",
       "      <th>phase</th>\n",
       "      <th>pass_filter</th>\n",
       "      <th>cell_name</th>\n",
       "      <th>plate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARCODE_SUB_LIB_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01_001_053-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>2671</td>\n",
       "      <td>5629</td>\n",
       "      <td>6830</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1119</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_053</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>-0.265873</td>\n",
       "      <td>-0.313553</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>CFPAC-1</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_082-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>2148</td>\n",
       "      <td>3173</td>\n",
       "      <td>3826</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0292</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_082</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.400794</td>\n",
       "      <td>0.520879</td>\n",
       "      <td>G2M</td>\n",
       "      <td>full</td>\n",
       "      <td>HCT15</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_145-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>683</td>\n",
       "      <td>886</td>\n",
       "      <td>1073</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_1098</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_145</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>-0.019841</td>\n",
       "      <td>-0.032967</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>HepG2/C3A</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_175-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>1845</td>\n",
       "      <td>2786</td>\n",
       "      <td>3368</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0131</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_175</td>\n",
       "      <td>0.031587</td>\n",
       "      <td>-0.123016</td>\n",
       "      <td>-0.118498</td>\n",
       "      <td>G1</td>\n",
       "      <td>full</td>\n",
       "      <td>A-172</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01_001_181-lib_1000</th>\n",
       "      <td>smp_1591</td>\n",
       "      <td>1228</td>\n",
       "      <td>1849</td>\n",
       "      <td>2226</td>\n",
       "      <td>[('Infigratinib', 0.5, 'uM')]</td>\n",
       "      <td>Infigratinib</td>\n",
       "      <td>CVCL_0399</td>\n",
       "      <td>lib_1000</td>\n",
       "      <td>01_001_181</td>\n",
       "      <td>0.015143</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>-0.008791</td>\n",
       "      <td>S</td>\n",
       "      <td>full</td>\n",
       "      <td>LoVo</td>\n",
       "      <td>plate2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sample  gene_count  tscp_count  mread_count  \\\n",
       "BARCODE_SUB_LIB_ID                                                   \n",
       "01_001_053-lib_1000  smp_1591        2671        5629         6830   \n",
       "01_001_082-lib_1000  smp_1591        2148        3173         3826   \n",
       "01_001_145-lib_1000  smp_1591         683         886         1073   \n",
       "01_001_175-lib_1000  smp_1591        1845        2786         3368   \n",
       "01_001_181-lib_1000  smp_1591        1228        1849         2226   \n",
       "\n",
       "                                 drugname_drugconc          drug  cell_line  \\\n",
       "BARCODE_SUB_LIB_ID                                                            \n",
       "01_001_053-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_1119   \n",
       "01_001_082-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0292   \n",
       "01_001_145-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_1098   \n",
       "01_001_175-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0131   \n",
       "01_001_181-lib_1000  [('Infigratinib', 0.5, 'uM')]  Infigratinib  CVCL_0399   \n",
       "\n",
       "                    sublibrary     BARCODE  pcnt_mito   S_score  G2M_score  \\\n",
       "BARCODE_SUB_LIB_ID                                                           \n",
       "01_001_053-lib_1000   lib_1000  01_001_053   0.016522 -0.265873  -0.313553   \n",
       "01_001_082-lib_1000   lib_1000  01_001_082   0.025843  0.400794   0.520879   \n",
       "01_001_145-lib_1000   lib_1000  01_001_145   0.029345 -0.019841  -0.032967   \n",
       "01_001_175-lib_1000   lib_1000  01_001_175   0.031587 -0.123016  -0.118498   \n",
       "01_001_181-lib_1000   lib_1000  01_001_181   0.015143  0.023810  -0.008791   \n",
       "\n",
       "                    phase pass_filter  cell_name   plate  \n",
       "BARCODE_SUB_LIB_ID                                        \n",
       "01_001_053-lib_1000    G1        full    CFPAC-1  plate2  \n",
       "01_001_082-lib_1000   G2M        full      HCT15  plate2  \n",
       "01_001_145-lib_1000    G1        full  HepG2/C3A  plate2  \n",
       "01_001_175-lib_1000    G1        full      A-172  plate2  \n",
       "01_001_181-lib_1000     S        full       LoVo  plate2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate2_adata.obs.head(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "Pvdt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 06:02:23,753\tINFO worker.py:2003 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-07 06:02:23,777\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example/notebooks'.\n",
      "2025-11-07 06:02:23,795\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_6ce7ea5a35b55e7c.zip' (2.58MiB) to Ray cluster...\n",
      "2025-11-07 06:02:23,815\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_6ce7ea5a35b55e7c.zip'.\n"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "trainer = RayTrainRunner(\n",
    "    LinearClassifier,\n",
    "    DistributedCellLineAnnDataset,\n",
    "    model_keys = [\"num_genes\",\n",
    "                  \"num_classes\"],\n",
    "    metadata_cb = cell_line_metadata_cb,\n",
    "    sparse_key = \"X\",\n",
    "    runtime_env_config = {\"working_dir\": os.getcwd()},\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ZBYS",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protoplast.scrna.anndata.trainer - INFO - Using 1 workers where each worker uses: {'CPU': 12, 'GPU': 1}\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of val_split: 98 length of test_split: 0, length of train_split: 394\n",
      "protoplast.scrna.anndata.strategy - INFO - Dropping 8 mini-batches\n",
      "protoplast.scrna.anndata.strategy - INFO - Dropping 4 mini-batches\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of after dropping remainder val_split: 98, length of test_split: 0, length of train_split: 394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=2980637)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainController pid=2980637)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=2980637)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=2980637)\u001b[0m Attempting to start training worker group of size 1 with the following resources: [{'CPU': 12, 'GPU': 1}] * 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=2980637)\u001b[0m Started training worker group of size 1: \n",
      "\u001b[36m(TrainController pid=2980637)\u001b[0m - (ip=192.168.1.226, pid=2980921) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return _C._get_float32_matmul_precision()\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m Restoring states from the checkpoint path at /home/dtran/training_results/ray_train_run-2025-11-07_05-59-18/checkpoint_2025-11-07_06-02-11.615409/checkpoint.ckpt\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:445: The dirpath has changed from '/tmp/ray/session_2025-11-07_05-58-58_829460_2954386/artifacts/ray_train_run-2025-11-07_05-59-18/lightning_logs/version_0/checkpoints' to '/tmp/ray/session_2025-11-07_06-02-17_556116_2954386/artifacts/ray_train_run-2025-11-07_06-02-42/lightning_logs/version_0/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   | Name    | Type             | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 0 | model   | Linear           | 3.1 M  | train\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 3.1 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 3.1 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 12.542    Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 2         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m Restored all states from the checkpoint at /home/dtran/training_results/ray_train_run-2025-11-07_05-59-18/checkpoint_2025-11-07_06-02-11.615409/checkpoint.ckpt\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   warnings.warn(  # warn only once\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:123: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/torch/multiprocessing/reductions.py:473: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_compressed_tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/protoplast/scrna/anndata/torch_dataloader.py:135: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m   return torch.sparse_csr_tensor(\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m /mnt/hdd1/dung/protoplast-ml-example/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \n",
      "Epoch 1:   0%|          | 0/6300 [00:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 3/6300 [00:22<13:08:33,  0.13it/s, v_num=0, train_loss=0.101] \n",
      "...\n",
      "...\n",
      "...\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 6293/6300 [03:17<00:00, 31.92it/s, v_num=0, train_loss=0.153] \n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6300/6300 [03:17<00:00, 31.94it/s, v_num=0, train_loss=0.108] \n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1560 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/1560 [00:00<00:37, 41.08it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m \n",
      "Validation DataLoader 0:   0%|          | 2/1560 [00:00<00:38, 40.40it/s]\u001b[A\n",
      "...\n",
      "...\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1559/1560 [00:30<00:00, 50.56it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1560/1560 [00:30<00:00, 50.57it/s]\u001b[A\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6300/6300 [04:09<00:00, 25.25it/s, v_num=0, train_loss=0.108]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6300/6300 [04:09<00:00, 25.23it/s, v_num=0, train_loss=0.108]\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6300/6300 [04:09<00:00, 25.22it/s, v_num=0, train_loss=0.108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-07_06-02-42/checkpoint_2025-11-07_06-07-35.887101)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-07_06-02-42/checkpoint_2025-11-07_06-07-35.887101), metrics={'train_loss': 0.10751987993717194, 'val_acc': 0.981541097164154, 'epoch': 1, 'step': 10584}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=2980921)\u001b[0m `Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(metrics={'train_loss': 0.10751987993717194, 'val_acc': 0.981541097164154, 'epoch': 1, 'step': 10584}, checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-07_06-02-42/checkpoint_2025-11-07_06-07-35.887101), error=None, path='/home/dtran/protoplast_results/ray_train_run-2025-11-07_06-02-42', metrics_dataframe=   train_loss   val_acc  epoch   step\n",
       "0     0.10752  0.981541      1  10584, best_checkpoints=[(Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-07_06-02-42/checkpoint_2025-11-07_06-07-35.887101), {'train_loss': 0.10751987993717194, 'val_acc': 0.981541097164154, 'epoch': 1, 'step': 10584})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7ac7398edcf0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the checkpoint path from the training result in Tahoe plate 1. The training\n",
    "# progress will be shown in cell 12 above.\n",
    "ckpt_path = os.path.join(result.checkpoint.path, \"checkpoint.ckpt\")\n",
    "\n",
    "trainer.train([DS_PATHS[1]],\n",
    "              max_epochs = 2,\n",
    "              batch_size = 1024,\n",
    "              test_size = test_size, \n",
    "              val_size = val_size,\n",
    "              num_workers = 1,\n",
    "              thread_per_worker = thread_per_worker,\n",
    "              ckpt_path = ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b219b13-18c6-4dbe-9cd8-fa2a69939502",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f424dd0-f4f3-40cb-bbdc-dcd9f7651852",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This brings us to the end of the tutorial notebook.\n",
    "\n",
    "This workflow highlights using checkpointing in **PROTOplast**, enabling efficient model development across diverse datasets.\n",
    "\n",
    "Feel free to explore and extend this notebook to suit your own data and use cases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e1214-d769-47c2-b5be-c26ce7f55b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
