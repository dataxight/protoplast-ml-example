{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Perturbation Models for Single-Cell Data with PROTOplast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "This notebook showcases **perturbation models** for the **Tahoe-100M** dataset, focusing on predicting gene expression changes under drug perturbations. We demonstrate two approaches: a statistical baseline and a neural embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a3cbd",
   "metadata": {},
   "source": [
    "**Download the Tahoe-100M `h5ad` files**\n",
    "- The Tahoe-100M dataset can be downloaded in `h5ad` format from the **Arc Institute Google Cloud Storage**. For step-by-step instructions, see the [official tutorial](https://github.com/ArcInstitute/arc-virtual-cell-atlas/blob/main/tahoe-100M/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {},
   "source": [
    "**Set up**\n",
    "- Set up the training environment for single-cell RNA sequencing (scRNA-seq) data using PROTOplast together with PyTorch Lightning and Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63e777d-58b6-4649-9dc6-337b044c1f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - Logging initialized. Current level is: INFO\n"
     ]
    }
   ],
   "source": [
    "import protoplast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766c5adb-ae9c-4e71-bb0f-b0f803142e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(version(\"protoplast\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lEQa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import numpy as np\n",
    "import torch\n",
    "from protoplast.scrna.anndata.torch_dataloader import DistributedAnnDataset, cell_line_metadata_cb\n",
    "from protoplast.scrna.anndata.trainer import RayTrainRunner\n",
    "\n",
    "# models from state\n",
    "from state.tx.models.embed_sum import EmbedSumPerturbationModel\n",
    "from state.tx.models.perturb_mean import PerturbMeanPerturbationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6e3f3",
   "metadata": {},
   "source": [
    "## 1. Load the Tahoe 100-M Dataset (`h5ad`)\n",
    "- `file_paths`: here, only Plate 12 from Tahoe-100M (The largest file: 35 GB) is used as a demo. To add more plates, append their `.h5ad` file paths to the list, separated by commas\n",
    "- `batch_size`: number of samples per training batch\n",
    "- `test_size`: fraction of data reserved for testing\n",
    "- `val_size`: fraction of data reserved for validation (use `0.0` if no validation set is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"/mnt/hdd2/tan/tahoe100m/plate12_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\"]\n",
    "batch_size = 2000\n",
    "test_size = 0.0\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Perturbation Mean\n",
    "**PerturbMeanPerturbationModel** (from STATE) is a *statistical baseline* that predicts perturbed expression by combining a control baseline (global or per-sample) with a perturbation-specific offset averaged across cell types.\n",
    "- **Inputs**\n",
    "    - Perturbation identifier\n",
    "    - Cell type (cell line in Tahoe-100M)\n",
    "    - Perturbed counts or embeddings\n",
    "    - (Optional) control embedding\n",
    "- **Output**\n",
    "    - Predicted gene expression profile (or latent embedding, depending on configuration)\n",
    "Note: This model is not trained so no learnable weights, no validation data). Its predictions come purely from statistics of the training dataset. \n",
    "**Source code:** [perturb_mean.py](https://github.com/ArcInstitute/state/blob/b6d26731e41d78c8c789d6973fe3d7db7853e9ad/src/state/tx/models/perturb_mean.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "### Metadata Callback\n",
    "The `perturbmean_metadata_cb` function prepares metadata for the **Perturbation Mean** model.  \n",
    "- It converts drug and cell line columns to categorical values, sets input/output dimensions, hidden size, perturbation dimension, and training hyperparameters.  \n",
    "- It also stores gene names, perturbation names, and cell types, while designating `DMSO_TF` as the control, `X` as the embedding key, and `gene` as the output space.\n",
    "- `perturbmean_metadata_cb` prepares metadata for the Perturbation Mean model. It casts drug and cell line columns to categorical, sets input/output dimensions, hidden size, and perturbation dimension, and defines training hyperparameters. It also records gene names, perturbation names, and cell types, while specifying `DMSO_TF` as the control, `X` as the embedding key, and `gene` as the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbmean_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    ad.obs[\"drug\"] = ad.obs[\"drug\"].astype(\"category\")\n",
    "    ad.obs[\"cell_line\"] = ad.obs[\"cell_line\"].astype(\"category\")\n",
    "\n",
    "    metadata[\"input_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"output_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"hidden_dim\"] = 0  # hidden_dim: Not used here, but required by base-class signature.\n",
    "    metadata[\"pert_dim\"] = ad.obs[\"drug\"].astype(str).nunique()\n",
    "    metadata[\"lr\"] = 1e-3\n",
    "\n",
    "    metadata[\"gene_names\"] = ad.var_names.tolist()\n",
    "    metadata[\"pert_names\"] = ad.obs[\"drug\"].cat.categories.tolist()\n",
    "    metadata[\"cell_types\"] = ad.obs[\"cell_line\"].cat.categories.tolist()\n",
    "    metadata[\"control_pert\"] = \"DMSO_TF\"\n",
    "    metadata[\"embed_key\"] = \"X\"\n",
    "    metadata[\"output_space\"] = \"gene\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "### Perturbation Dataset for Training (PerturbAnnDataset)\n",
    "`PerturbAnnDataset` prepares batches for the **Perturbation Mean** model. It loads expression data, collects `drug` and `cell_line` metadata, and returns a dictionary containing perturbation names, cell types, and the corresponding expression features (used both as counts and embeddings) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbAnnDataset(DistributedAnnDataset):\n",
    "    def transform(self, start: int, end: int):\n",
    "        X = super().transform(start, end)\n",
    "\n",
    "        # Metadata froms self.ad\n",
    "        pert_names = self.ad.obs[\"drug\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = self.ad.obs[\"cell_line\"].iloc[start:end].astype(str).to_list()\n",
    "\n",
    "        return {\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "            \"pert_cell_counts\": X,\n",
    "            \"pert_cell_emb\": X,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "### Extending STATE Models\n",
    "The **STATE** framework provides baseline model classes such as `PerturbMeanPerturbationModel`, which can be imported and used directly.\n",
    "To customize behavior, you can **extend an existing class** and override only the methods that need modification.  \n",
    "In the example below, we subclass `PerturbMeanPerturbationModel` and redefine the `forward()` method. Rather than relying on the per-cell `ctrl_cell_emb`, the model predicts using only the **global basal vector** combined with the corresponding perturbation offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbMeanGlobalModel(PerturbMeanPerturbationModel):\n",
    "    \"\"\"\n",
    "    Extended class of PerturbMeanPerturbationModel where prediction ignores\n",
    "    per-cell control embedding and uses only global basal + offset.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        B = len(batch[\"pert_name\"])\n",
    "        device = self.dummy_param.device\n",
    "        pred_out = torch.zeros((B, self.output_dim), device=device)\n",
    "\n",
    "        for i in range(B):\n",
    "            p_name = str(batch[\"pert_name\"][i])\n",
    "            offset_vec = self.pert_mean_offsets.get(p_name)\n",
    "            if offset_vec is None:\n",
    "                offset_vec = torch.zeros(self.output_dim, device=device)\n",
    "\n",
    "            # Use global basal instead of batch[\"ctrl_cell_emb\"]\n",
    "            pred_out[i] = self.global_basal.to(device) + offset_vec.to(device)\n",
    "\n",
    "        return pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e38aa",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "- Collect statistics (`on_fit_start`)\n",
    "    - Compute control means per cell type\n",
    "    - Compute perturbation deltas\n",
    "    - Average deltas across cell types â†’ perturbation offsets\n",
    "    - Compute global basal = mean of all control means.\n",
    "- Forward: for each sample, `prediction = global_basal + offset[perturbation]`\n",
    "- Training: no parameters are learned; only logs MSE loss vs. ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "iLit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 17:21:11,603\tINFO worker.py:2003 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-04 17:21:11,692\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example/notebooks'.\n",
      "2025-11-04 17:21:11,720\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_30a635a9689a3416.zip' (3.58MiB) to Ray cluster...\n",
      "2025-11-04 17:21:11,741\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_30a635a9689a3416.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 339 ms, sys: 315 ms, total: 654 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PerturbMeanPerturbationModel_trainer = RayTrainRunner(\n",
    "    PerturbMeanGlobalModel,\n",
    "    PerturbAnnDataset,\n",
    "    [\n",
    "        \"input_dim\",\n",
    "        \"output_dim\",\n",
    "        \"hidden_dim\",\n",
    "        \"pert_dim\",\n",
    "        \"lr\",\n",
    "        \"control_pert\",  # \"DMSO_TF\"\n",
    "        \"embed_key\",\n",
    "        \"output_space\",  # \"gene\"\n",
    "    ],\n",
    "    perturbmean_metadata_cb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11ae35",
   "metadata": {},
   "source": [
    "On a machine with **1 GPU (NVIDIA GeForce RTX 3080 - 12 GiB)**, **96 CPUs**, and **125 GiB RAM**, running `PerturbMeanPerturbationModel_trainer.train()` completed in approximately **47 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ZHCJ",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protoplast.scrna.anndata.trainer - INFO - Setting thread_per_worker to half of the available CPUs capped at 4\n",
      "protoplast.scrna.anndata.trainer - INFO - Using 1 workers where each worker uses: {'CPU': 4, 'GPU': 1}\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of val_split: 65 length of test_split: 0, length of train_split: 262\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of after dropping remainder val_split: 65, length of test_split: 0, length of train_split: 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=1568983)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainController pid=1568983)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=1568983)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=1568983)\u001b[0m Attempting to start training worker group of size 1 with the following resources: [{'CPU': 4, 'GPU': 1}] * 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=1568983)\u001b[0m Started training worker group of size 1: \n",
      "\u001b[36m(TrainController pid=1568983)\u001b[0m - (ip=192.168.1.226, pid=1569566) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m   | Name         | Type    | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 0 | loss_fn      | MSELoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m   | other params | n/a     | 1      | n/a  \n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 1         Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 1         Total params\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 0.000     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 1         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m 0         Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m PerturbMean: computed offsets for 95 perturbations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.58it/s]\n",
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.08it/s]\n",
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/4192 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 1/4192 [00:27<32:05:11,  0.04it/s, v_num=0, train_loss=0.406]\n",
      "...\n",
      "...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4191/4192 [30:43<00:00,  2.27it/s, v_num=0, train_loss=0.582]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [30:44<00:00,  2.27it/s, v_num=0, train_loss=0.268]\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1040 [00:00<?, ?it/s]\u001b[A\n",
      "...\n",
      "...\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1040/1040 [07:26<00:00,  2.33it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [38:35<00:00,  1.81it/s, v_num=0, train_loss=0.268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m PerturbMean: Saved global_basal and pert_mean_offsets to checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [38:35<00:00,  1.81it/s, v_num=0, train_loss=0.268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_17-21-43/checkpoint_2025-11-04_18-08-23.026793)\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_17-21-43/checkpoint_2025-11-04_18-08-23.026793), metrics={'train_loss': 0.26821431517601013, 'val_loss': 0.5752615928649902, 'epoch': 0, 'step': 4192}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m PerturbMean: Saved global_basal and pert_mean_offsets to checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [38:36<00:00,  1.81it/s, v_num=0, train_loss=0.268]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1569566)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.7 s, sys: 14.2 s, total: 54.9 s\n",
      "Wall time: 47min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(metrics={'train_loss': 0.26821431517601013, 'val_loss': 0.5752615928649902, 'epoch': 0, 'step': 4192}, checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_17-21-43/checkpoint_2025-11-04_18-08-23.026793), error=None, path='/home/dtran/protoplast_results/ray_train_run-2025-11-04_17-21-43', metrics_dataframe=   train_loss  val_loss  epoch  step\n",
       "0    0.268214  0.575262      0  4192, best_checkpoints=[(Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_17-21-43/checkpoint_2025-11-04_18-08-23.026793), {'train_loss': 0.26821431517601013, 'val_loss': 0.5752615928649902, 'epoch': 0, 'step': 4192})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7dee4c6561f0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "PerturbMeanPerturbationModel_trainer.train(\n",
    "    file_paths,\n",
    "    batch_size,  # 2000\n",
    "    test_size,  # 0.0\n",
    "    val_size,  # 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb929268-60a8-4650-89a2-f906e17e0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "## 3. EmbedSum\n",
    "The **EmbedSumPerturbationModel** (part of the STATE framework) is a neural embedding model that predicts gene expression under perturbations.  \n",
    "It works by combining a **control (basal) cell state** with a **learned perturbation embedding**.  \n",
    "**Inputs**\n",
    "  - Control (basal) expression counts or embedding  \n",
    "  - Perturbation one-hot vector  \n",
    "\n",
    "**Output**\n",
    "  - Predicted gene expression profile  \n",
    "**Source code:** [embed_sum.py](https://github.com/ArcInstitute/state/blob/b6d26731e41d78c8c789d6973fe3d7db7853e9ad/src/state/tx/models/embed_sum.py#L7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "### Metadata Callback\n",
    "The `embedsum_metadata_cb` function prepares metadata for the `EmbedSumPerturbationModel`. It sets the input and output dimensions (equal to the **number of genes**), defines the perturbation dimension based on the unique drugs in the dataset, and specifies training parameters such as hidden layer size and the control perturbation (`DMSO_TF`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedsum_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    cell_line_metadata_cb(ad, metadata)\n",
    "    metadata[\"input_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"output_dim\"] = ad.var.shape[0]\n",
    "\n",
    "    uniq_drugs = sorted(ad.obs[\"drug\"].astype(str).unique().tolist())\n",
    "    metadata[\"pert_dim\"] = len(uniq_drugs)\n",
    "\n",
    "    metadata[\"hidden_dim\"] = 10  # here kept small for testing\n",
    "    metadata[\"control_pert\"] = \"DMSO_TF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "### EmbedSumAnnDataset\n",
    "The `EmbedSumAnnDataset` class extends `DistributedAnnDataset` and prepares batches for the `EmbedSumPerturbationModel`. It enriches each batch with drug embeddings, metadata, and control information needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedSumAnnDataset(DistributedAnnDataset):\n",
    "    control_drug = \"DMSO_TF\"\n",
    "\n",
    "    def transform(self, start: int, end: int):\n",
    "        # Loads gene expression (X) and converts it into a tensor (target_gene_expr).\n",
    "        X = super().transform(start, end)\n",
    "        target_gene_expr = torch.as_tensor(X, dtype=torch.float32)\n",
    "        device = target_gene_expr.device\n",
    "\n",
    "        # Collects metadata: perturbation names (drug) and cell line labels\n",
    "        pert_names = self.ad.obs[\"drug\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = self.ad.obs[\"cell_line\"].iloc[start:end].astype(str).to_list()\n",
    "\n",
    "        # Create drug index mapping\n",
    "        if not hasattr(self, \"_drug_to_idx\"):\n",
    "            drug_names = sorted(self.ad.obs[\"drug\"].astype(str).unique())\n",
    "            self._drug_to_idx = {d: i for i, d in enumerate(drug_names)}\n",
    "            self._num_drugs = len(drug_names)\n",
    "\n",
    "        # encodes drugs as one-hot embeddings\n",
    "        idxs = [self._drug_to_idx.get(p, 0) for p in pert_names]\n",
    "        pert_emb = torch.nn.functional.one_hot(torch.tensor(idxs, device=device), num_classes=self._num_drugs).float()\n",
    "\n",
    "        # Computes a global control mean expression vector from cells treated with DMSO_TF\n",
    "        if not hasattr(self, \"_ctrl_global\"):\n",
    "            mask = self.ad.obs[\"drug\"] == self.control_drug\n",
    "            if mask.sum() == 0:\n",
    "                ctrl_vec = np.zeros(self.ad.shape[1], dtype=np.float32)\n",
    "            else:\n",
    "                ctrl_vec = np.asarray(self.ad[mask].X.mean(axis=0)).ravel().astype(np.float32)\n",
    "            self._ctrl_global = torch.from_numpy(ctrl_vec)\n",
    "\n",
    "        ctrl_cell_emb = self._ctrl_global.to(device).unsqueeze(0).expand(len(pert_names), -1)\n",
    "\n",
    "        # Returns a dictionary containing embeddings, control features, target expression, and metadata for perturbation training\n",
    "        return {\n",
    "            \"pert_emb\": pert_emb,\n",
    "            \"ctrl_cell_emb\": ctrl_cell_emb,\n",
    "            \"target_gene_expr\": target_gene_expr,\n",
    "            \"pert_cell_emb\": target_gene_expr,\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "### Training the EmbedSumPerturbationModel\n",
    "- It pairs the model with the custom `EmbedSumAnnDataset` and passes in required arguments (dimensions, learning rate, control perturbation, embedding key, and output space) via `embedsum_metadata_cb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecfG",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 18:08:34,716\tINFO worker.py:2003 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-11-04 18:08:34,843\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example/notebooks'.\n",
      "2025-11-04 18:08:34,871\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_5c046a9a69b2a95d.zip' (3.07MiB) to Ray cluster...\n",
      "2025-11-04 18:08:34,891\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_5c046a9a69b2a95d.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 ms, sys: 326 ms, total: 536 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EmbedSumPerturbationModel_trainer = RayTrainRunner(\n",
    "    EmbedSumPerturbationModel,\n",
    "    EmbedSumAnnDataset,\n",
    "    [\n",
    "        \"input_dim\",\n",
    "        \"output_dim\",\n",
    "        \"hidden_dim\",\n",
    "        \"pert_dim\",\n",
    "        \"lr\",\n",
    "        \"control_pert\",  # \"DMSO_TF\"\n",
    "        \"embed_key\",\n",
    "        \"output_space\",  # \"gene\"\n",
    "    ],\n",
    "    embedsum_metadata_cb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff4544",
   "metadata": {},
   "source": [
    "On a machine with **1 GPU (NVIDIA GeForce RTX 3080 - 12 GiB)**, **96 CPUs**, and **125 GiB RAM**, running `EmbedSumPerturbationModel_trainer.train()` completed in approximately **28 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Pvdt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "protoplast.scrna.anndata.trainer - INFO - Setting thread_per_worker to half of the available CPUs capped at 4\n",
      "protoplast.scrna.anndata.trainer - INFO - Using 1 workers where each worker uses: {'CPU': 4, 'GPU': 1}\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of val_split: 65 length of test_split: 0, length of train_split: 262\n",
      "protoplast.scrna.anndata.strategy - INFO - Length of after dropping remainder val_split: 65, length of test_split: 0, length of train_split: 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=1595724)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainController pid=1595724)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainController pid=1595724)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=1595724)\u001b[0m Attempting to start training worker group of size 1 with the following resources: [{'CPU': 4, 'GPU': 1}] * 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m root - INFO - Logging initialized. Current level is: INFO\n",
      "\u001b[36m(TrainController pid=1595724)\u001b[0m Started training worker group of size 1: \n",
      "\u001b[36m(TrainController pid=1595724)\u001b[0m - (ip=192.168.1.226, pid=1596111) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m   | Name          | Type       | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 0 | loss_fn       | MSELoss    | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 1 | pert_encoder  | Sequential | 1.1 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 2 | basal_encoder | Sequential | 627 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 3 | project_out   | Sequential | 689 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 1.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 1.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 5.273     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 16        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  1.19it/s]\n",
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/4192 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 1/4192 [01:19<92:52:41,  0.01it/s, v_num=0]\n",
      "...\n",
      "...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [20:29<00:00,  3.41it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "...\n",
      "...\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [25:46<00:00,  2.71it/s, v_num=0]       \u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [25:46<00:00,  2.71it/s, v_num=0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4192/4192 [25:46<00:00,  2.71it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_18-09-02/checkpoint_2025-11-04_18-36-31.757231)\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m Reporting training result 1: TrainingReport(checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_18-09-02/checkpoint_2025-11-04_18-36-31.757231), metrics={'train_loss': 0.29701322317123413, 'val_loss': 0.5971354246139526, 'epoch': 0, 'step': 4192}, validation_spec=None)\n",
      "\u001b[36m(RayTrainWorker pid=1596111)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 9.78 s, total: 46.7 s\n",
      "Wall time: 27min 56s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(metrics={'train_loss': 0.29701322317123413, 'val_loss': 0.5971354246139526, 'epoch': 0, 'step': 4192}, checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_18-09-02/checkpoint_2025-11-04_18-36-31.757231), error=None, path='/home/dtran/protoplast_results/ray_train_run-2025-11-04_18-09-02', metrics_dataframe=   train_loss  val_loss  epoch  step\n",
       "0    0.297013  0.597135      0  4192, best_checkpoints=[(Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/ray_train_run-2025-11-04_18-09-02/checkpoint_2025-11-04_18-36-31.757231), {'train_loss': 0.29701322317123413, 'val_loss': 0.5971354246139526, 'epoch': 0, 'step': 4192})], _storage_filesystem=<pyarrow._fs.LocalFileSystem object at 0x7de4f8da1ff0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "EmbedSumPerturbationModel_trainer.train(\n",
    "    file_paths,\n",
    "    batch_size,  # 2000\n",
    "    test_size,  # 0.0\n",
    "    val_size,  # 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701b7c6-d275-42a9-9819-8ed73b04d218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
