{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "MJUe",
   "metadata": {},
   "source": [
    "# Perturbation Models for Single-Cell Data with PROTOplast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vblA",
   "metadata": {},
   "source": [
    "This notebook showcases **perturbation models** for the **Tahoe-100M** dataset, focusing on predicting gene expression changes under drug perturbations. We demonstrate two approaches: a statistical baseline and a neural embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a3cbd",
   "metadata": {},
   "source": [
    "**Download the Tahoe-100M `h5ad` files**\n",
    "- The Tahoe-100M dataset can be downloaded in `h5ad` format from the **Arc Institute Google Cloud Storage**. For step-by-step instructions, see the [official tutorial](https://github.com/ArcInstitute/arc-virtual-cell-atlas/blob/main/tahoe-100M/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bkHC",
   "metadata": {},
   "source": [
    "**Set up**\n",
    "- Set up the training environment for single-cell RNA sequencing (scRNA-seq) data using PROTOplast together with PyTorch Lightning and Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import numpy as np\n",
    "import torch\n",
    "from protoplast.scrna.anndata.torch_dataloader import DistributedAnnDataset, cell_line_metadata_cb\n",
    "from protoplast.scrna.anndata.trainer import RayTrainRunner\n",
    "\n",
    "# models from state\n",
    "from state.tx.models.embed_sum import EmbedSumPerturbationModel\n",
    "from state.tx.models.perturb_mean import PerturbMeanPerturbationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb6e3f3",
   "metadata": {},
   "source": [
    "## 1. Load the Tahoe 100-M Dataset (`h5ad`)\n",
    "- `file_paths`: here, only Plate 12 from Tahoe-100M (The largest file: 35 GB) is used as a demo. To add more plates, append their `.h5ad` file paths to the list, separated by commas\n",
    "- `batch_size`: number of samples per training batch\n",
    "- `test_size`: fraction of data reserved for testing\n",
    "- `val_size`: fraction of data reserved for validation (use `0.0` if no validation set is needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"/mnt/hdd2/tan/tahoe100m/plate12_filt_Vevo_Tahoe100M_WServicesFrom_ParseGigalab.h5ad\"]\n",
    "batch_size = 2000\n",
    "test_size = 0.0\n",
    "val_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SFPL",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "## 2. Perturbation Mean\n",
    "**PerturbMeanPerturbationModel** (from STATE) is a *statistical baseline* that predicts perturbed expression by combining a control baseline (global or per-sample) with a perturbation-specific offset averaged across cell types.\n",
    "- **Inputs**\n",
    "    - Perturbation identifier\n",
    "    - Cell type (cell line in Tahoe-100M)\n",
    "    - Perturbed counts or embeddings\n",
    "    - (Optional) control embedding\n",
    "- **Output**\n",
    "    - Predicted gene expression profile (or latent embedding, depending on configuration)\n",
    "Note: This model is not trained so no learnable weights, no validation data). Its predictions come purely from statistics of the training dataset. \n",
    "**Source code:** [perturb_mean.py](https://github.com/ArcInstitute/state/blob/b6d26731e41d78c8c789d6973fe3d7db7853e9ad/src/state/tx/models/perturb_mean.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BYtC",
   "metadata": {},
   "source": [
    "### Metadata Callback\n",
    "The `perturbmean_metadata_cb` function prepares metadata for the **Perturbation Mean** model.  \n",
    "- It converts drug and cell line columns to categorical values, sets input/output dimensions, hidden size, perturbation dimension, and training hyperparameters.  \n",
    "- It also stores gene names, perturbation names, and cell types, while designating `DMSO_TF` as the control, `X` as the embedding key, and `gene` as the output space.\n",
    "- `perturbmean_metadata_cb` prepares metadata for the Perturbation Mean model. It casts drug and cell line columns to categorical, sets input/output dimensions, hidden size, and perturbation dimension, and defines training hyperparameters. It also records gene names, perturbation names, and cell types, while specifying `DMSO_TF` as the control, `X` as the embedding key, and `gene` as the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "RGSE",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturbmean_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    ad.obs[\"drug\"] = ad.obs[\"drug\"].astype(\"category\")\n",
    "    ad.obs[\"cell_line\"] = ad.obs[\"cell_line\"].astype(\"category\")\n",
    "\n",
    "    metadata[\"input_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"output_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"hidden_dim\"] = 0  # hidden_dim: Not used here, but required by base-class signature.\n",
    "    metadata[\"pert_dim\"] = ad.obs[\"drug\"].astype(str).nunique()\n",
    "    metadata[\"lr\"] = 1e-3\n",
    "\n",
    "    metadata[\"gene_names\"] = ad.var_names.tolist()\n",
    "    metadata[\"pert_names\"] = ad.obs[\"drug\"].cat.categories.tolist()\n",
    "    metadata[\"cell_types\"] = ad.obs[\"cell_line\"].cat.categories.tolist()\n",
    "    metadata[\"control_pert\"] = \"DMSO_TF\"\n",
    "    metadata[\"embed_key\"] = \"X\"\n",
    "    metadata[\"output_space\"] = \"gene\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kclp",
   "metadata": {},
   "source": [
    "### Perturbation Dataset for Training (PerturbAnnDataset)\n",
    "`PerturbAnnDataset` prepares batches for the **Perturbation Mean** model. It loads expression data, collects `drug` and `cell_line` metadata, and returns a dictionary containing perturbation names, cell types, and the corresponding expression features (used both as counts and embeddings) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbAnnDataset(DistributedAnnDataset):\n",
    "    def transform(self, start: int, end: int):\n",
    "        X = super().transform(start, end)\n",
    "\n",
    "        # Metadata froms self.ad\n",
    "        pert_names = self.ad.obs[\"drug\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = self.ad.obs[\"cell_line\"].iloc[start:end].astype(str).to_list()\n",
    "\n",
    "        return {\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "            \"pert_cell_counts\": X,\n",
    "            \"pert_cell_emb\": X,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hstk",
   "metadata": {},
   "source": [
    "### Extending STATE Models\n",
    "The **STATE** framework provides baseline model classes such as `PerturbMeanPerturbationModel`, which can be imported and used directly.\n",
    "To customize behavior, you can **extend an existing class** and override only the methods that need modification.  \n",
    "In the example below, we subclass `PerturbMeanPerturbationModel` and redefine the `forward()` method. Rather than relying on the per-cell `ctrl_cell_emb`, the model predicts using only the **global basal vector** combined with the corresponding perturbation offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nWHF",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerturbMeanGlobalModel(PerturbMeanPerturbationModel):\n",
    "    \"\"\"\n",
    "    Extended class of PerturbMeanPerturbationModel where prediction ignores\n",
    "    per-cell control embedding and uses only global basal + offset.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, batch: dict) -> torch.Tensor:\n",
    "        B = len(batch[\"pert_name\"])\n",
    "        device = self.dummy_param.device\n",
    "        pred_out = torch.zeros((B, self.output_dim), device=device)\n",
    "\n",
    "        for i in range(B):\n",
    "            p_name = str(batch[\"pert_name\"][i])\n",
    "            offset_vec = self.pert_mean_offsets.get(p_name)\n",
    "            if offset_vec is None:\n",
    "                offset_vec = torch.zeros(self.output_dim, device=device)\n",
    "\n",
    "            # Use global basal instead of batch[\"ctrl_cell_emb\"]\n",
    "            pred_out[i] = self.global_basal.to(device) + offset_vec.to(device)\n",
    "\n",
    "        return pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0e38aa",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "- Collect statistics (`on_fit_start`)\n",
    "    - Compute control means per cell type\n",
    "    - Compute perturbation deltas\n",
    "    - Average deltas across cell types â†’ perturbation offsets\n",
    "    - Compute global basal = mean of all control means.\n",
    "- Forward: for each sample, `prediction = global_basal + offset[perturbation]`\n",
    "- Training: no parameters are learned; only logs MSE loss vs. ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "iLit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 16:31:51,373\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "2025-09-29 16:31:52,487\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example'.\n",
      "2025-09-29 16:31:52,625\tWARNING packaging.py:430 -- File /mnt/hdd1/dung/protoplast-ml-example/.git/modules/submodules/SIMS/objects/pack/pack-682433dc4cf8becc2b44606f464dde9068565261.pack is very large (34.70MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/mnt/hdd1/dung/protoplast-ml-example/.git/modules/submodules/SIMS/objects/pack/pack-682433dc4cf8becc2b44606f464dde9068565261.pack']})`\n",
      "2025-09-29 16:31:52,853\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_1f721884ccb69d33.zip' (70.69MiB) to Ray cluster...\n",
      "2025-09-29 16:31:53,343\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_1f721884ccb69d33.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 922 ms, sys: 799 ms, total: 1.72 s\n",
      "Wall time: 10.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Using CPython \u001b[36m3.11.13\u001b[39m\n",
      "\u001b[33m(raylet)\u001b[0m Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "\u001b[33m(raylet)\u001b[0m \u001b[2mInstalled \u001b[1m296 packages\u001b[0m \u001b[2min 377ms\u001b[0m\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=3371578)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainTrainable pid=3371578)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=3371578)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3371578)\u001b[0m - (node_id=b8b683fd763132fb5be6d98f0e4d56e917dd267ccfd076f40669efc5, ip=192.168.1.226, pid=3372413) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m =========Starting the training on 0 with num threads: 4=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m   | Name         | Type    | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 0 | loss_fn      | MSELoss | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m   | other params | n/a     | 1      | n/a  \n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m -------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 1         Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 1         Total params\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 0.000     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 1         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.93it/s]\n",
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/4160 [00:00<?, ?it/s] \n",
      "Epoch 0:   0%|          | 1/4160 [00:23<27:27:04,  0.04it/s, v_num=0, train_loss=0.245]\n",
      "Epoch 0:   0%|          | 2/4160 [00:24<13:54:30,  0.08it/s, v_num=0, train_loss=0.406]\n",
      ".\n",
      ".\n",
      ".\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4158/4160 [24:51<00:00,  2.79it/s, v_num=0, train_loss=0.365]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4159/4160 [24:52<00:00,  2.79it/s, v_num=0, train_loss=0.996]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [24:52<00:00,  2.79it/s, v_num=0, train_loss=0.474]\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Validation:   0%|          | 0/1024 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1024 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Validation DataLoader 0:   0%|          | 1/1024 [00:00<05:03,  3.37it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      ".\n",
      ".\n",
      ".\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1022/1024 [05:26<00:00,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1023/1024 [05:27<00:00,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [05:27<00:00,  3.13it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m \n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [30:51<00:00,  2.25it/s, v_num=0, train_loss=0.474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/TorchTrainer_2025-09-29_16-32-21/TorchTrainer_dfadf_00000_0_2025-09-29_16-32-22/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [30:53<00:00,  2.24it/s, v_num=0, train_loss=0.474]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [30:53<00:00,  2.24it/s, v_num=0, train_loss=0.474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3372413)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "PerturbMeanPerturbationModel_trainer = RayTrainRunner(\n",
    "    PerturbMeanGlobalModel,\n",
    "    PerturbAnnDataset,\n",
    "    [\n",
    "        \"input_dim\",\n",
    "        \"output_dim\",\n",
    "        \"hidden_dim\",\n",
    "        \"pert_dim\",\n",
    "        \"lr\",\n",
    "        \"control_pert\",  # \"DMSO_TF\"\n",
    "        \"embed_key\",\n",
    "        \"output_space\",  # \"gene\"\n",
    "    ],\n",
    "    perturbmean_metadata_cb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11ae35",
   "metadata": {},
   "source": [
    "On a machine with **1 GPU (NVIDIA GeForce RTX 3080 - 12 GiB)**, **96 CPUs**, and **125 GiB RAM**, running `PerturbMeanPerturbationModel_trainer.train()` completed in approximately **39 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ZHCJ",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting thread_per_worker to half of the available CPUs capped at 4\n",
      "Using 1 workers with {'CPU': 4} each\n",
      "=========Length of val_split 65 length of test_split 0 length of train_split 262\n",
      "=========Length of after dropping remainder val_split 64 length of test_split 0 length of train_split 260\n",
      "Data splitting time: 24.26 seconds\n",
      "Spawning Ray worker and initiating distributed training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 16:32:21,891\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-29 16:32:22 (running for 00:00:00.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/96 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_16-31-46_659688_3349722/artifacts/2025-09-29_16-32-21/TorchTrainer_2025-09-29_16-32-21/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-29 16:32:52 (running for 00:00:30.46)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_16-31-46_659688_3349722/artifacts/2025-09-29_16-32-21/TorchTrainer_2025-09-29_16-32-21/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-29 17:10:45 (running for 00:38:23.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_16-31-46_659688_3349722/artifacts/2025-09-29_16-32-21/TorchTrainer_2025-09-29_16-32-21/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:10:50,421\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/dtran/protoplast_results/TorchTrainer_2025-09-29_16-32-21' in 0.1060s.\n",
      "2025-09-29 17:10:50,471\tINFO tune.py:1041 -- Total run time: 2308.58 seconds (2307.96 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-29 17:10:50 (running for 00:38:28.07)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_16-31-46_659688_3349722/artifacts/2025-09-29_16-32-21/TorchTrainer_2025-09-29_16-32-21/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "CPU times: user 55.3 s, sys: 13.4 s, total: 1min 8s\n",
      "Wall time: 38min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'train_loss': 0.47350403666496277, 'val_loss': 0.5732423067092896, 'epoch': 0, 'step': 4160},\n",
       "  path='/home/dtran/protoplast_results/TorchTrainer_2025-09-29_16-32-21/TorchTrainer_dfadf_00000_0_2025-09-29_16-32-22',\n",
       "  filesystem='local',\n",
       "  checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/TorchTrainer_2025-09-29_16-32-21/TorchTrainer_dfadf_00000_0_2025-09-29_16-32-22/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "PerturbMeanPerturbationModel_trainer.train(\n",
    "    file_paths,\n",
    "    batch_size,  # 2000\n",
    "    test_size,  # 0.0\n",
    "    val_size,  # 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb929268-60a8-4650-89a2-f906e17e0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ROlb",
   "metadata": {},
   "source": [
    "## 3. EmbedSum\n",
    "The **EmbedSumPerturbationModel** (part of the STATE framework) is a neural embedding model that predicts gene expression under perturbations.  \n",
    "It works by combining a **control (basal) cell state** with a **learned perturbation embedding**.  \n",
    "**Inputs**\n",
    "  - Control (basal) expression counts or embedding  \n",
    "  - Perturbation one-hot vector  \n",
    "\n",
    "**Output**\n",
    "  - Predicted gene expression profile  \n",
    "**Source code:** [embed_sum.py](https://github.com/ArcInstitute/state/blob/b6d26731e41d78c8c789d6973fe3d7db7853e9ad/src/state/tx/models/embed_sum.py#L7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "### Metadata Callback\n",
    "The `embedsum_metadata_cb` function prepares metadata for the `EmbedSumPerturbationModel`. It sets the input and output dimensions (equal to the **number of genes**), defines the perturbation dimension based on the unique drugs in the dataset, and specifies training parameters such as hidden layer size and the control perturbation (`DMSO_TF`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "TqIu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedsum_metadata_cb(ad: anndata.AnnData, metadata: dict):\n",
    "    cell_line_metadata_cb(ad, metadata)\n",
    "    metadata[\"input_dim\"] = ad.var.shape[0]\n",
    "    metadata[\"output_dim\"] = ad.var.shape[0]\n",
    "\n",
    "    uniq_drugs = sorted(ad.obs[\"drug\"].astype(str).unique().tolist())\n",
    "    metadata[\"pert_dim\"] = len(uniq_drugs)\n",
    "\n",
    "    metadata[\"hidden_dim\"] = 10  # here kept small for testing\n",
    "    metadata[\"control_pert\"] = \"DMSO_TF\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vxnm",
   "metadata": {},
   "source": [
    "### EmbedSumAnnDataset\n",
    "The `EmbedSumAnnDataset` class extends `DistributedAnnDataset` and prepares batches for the `EmbedSumPerturbationModel`. It enriches each batch with drug embeddings, metadata, and control information needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedSumAnnDataset(DistributedAnnDataset):\n",
    "    control_drug = \"DMSO_TF\"\n",
    "\n",
    "    def transform(self, start: int, end: int):\n",
    "        # Loads gene expression (X) and converts it into a tensor (target_gene_expr).\n",
    "        X = super().transform(start, end)\n",
    "        target_gene_expr = torch.as_tensor(X, dtype=torch.float32)\n",
    "        device = target_gene_expr.device\n",
    "\n",
    "        # Collects metadata: perturbation names (drug) and cell line labels\n",
    "        pert_names = self.ad.obs[\"drug\"].iloc[start:end].astype(str).to_list()\n",
    "        cell_lines = self.ad.obs[\"cell_line\"].iloc[start:end].astype(str).to_list()\n",
    "\n",
    "        # Create drug index mapping\n",
    "        if not hasattr(self, \"_drug_to_idx\"):\n",
    "            drug_names = sorted(self.ad.obs[\"drug\"].astype(str).unique())\n",
    "            self._drug_to_idx = {d: i for i, d in enumerate(drug_names)}\n",
    "            self._num_drugs = len(drug_names)\n",
    "\n",
    "        # encodes drugs as one-hot embeddings\n",
    "        idxs = [self._drug_to_idx.get(p, 0) for p in pert_names]\n",
    "        pert_emb = torch.nn.functional.one_hot(torch.tensor(idxs, device=device), num_classes=self._num_drugs).float()\n",
    "\n",
    "        # Computes a global control mean expression vector from cells treated with DMSO_TF\n",
    "        if not hasattr(self, \"_ctrl_global\"):\n",
    "            mask = self.ad.obs[\"drug\"] == self.control_drug\n",
    "            if mask.sum() == 0:\n",
    "                ctrl_vec = np.zeros(self.ad.shape[1], dtype=np.float32)\n",
    "            else:\n",
    "                ctrl_vec = np.asarray(self.ad[mask].X.mean(axis=0)).ravel().astype(np.float32)\n",
    "            self._ctrl_global = torch.from_numpy(ctrl_vec)\n",
    "\n",
    "        ctrl_cell_emb = self._ctrl_global.to(device).unsqueeze(0).expand(len(pert_names), -1)\n",
    "\n",
    "        # Returns a dictionary containing embeddings, control features, target expression, and metadata for perturbation training\n",
    "        return {\n",
    "            \"pert_emb\": pert_emb,\n",
    "            \"ctrl_cell_emb\": ctrl_cell_emb,\n",
    "            \"target_gene_expr\": target_gene_expr,\n",
    "            \"pert_cell_emb\": target_gene_expr,\n",
    "            \"pert_name\": pert_names,\n",
    "            \"cell_type\": cell_lines,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "### Training the EmbedSumPerturbationModel\n",
    "- It pairs the model with the custom `EmbedSumAnnDataset` and passes in required arguments (dimensions, learning rate, control perturbation, embedding key, and output space) via `embedsum_metadata_cb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecfG",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:13:50,011\tINFO worker.py:1951 -- Started a local Ray instance.\n",
      "2025-09-29 17:13:50,728\tINFO packaging.py:588 -- Creating a file package for local module '/mnt/hdd1/dung/protoplast-ml-example'.\n",
      "2025-09-29 17:13:50,874\tWARNING packaging.py:430 -- File /mnt/hdd1/dung/protoplast-ml-example/.git/modules/submodules/SIMS/objects/pack/pack-682433dc4cf8becc2b44606f464dde9068565261.pack is very large (34.70MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/mnt/hdd1/dung/protoplast-ml-example/.git/modules/submodules/SIMS/objects/pack/pack-682433dc4cf8becc2b44606f464dde9068565261.pack']})`\n",
      "2025-09-29 17:13:51,105\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_3e11bd015efed140.zip' (70.55MiB) to Ray cluster...\n",
      "2025-09-29 17:13:51,625\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_3e11bd015efed140.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 679 ms, sys: 748 ms, total: 1.43 s\n",
      "Wall time: 11.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m Using CPython \u001b[36m3.11.13\u001b[39m\n",
      "\u001b[33m(raylet)\u001b[0m Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "\u001b[33m(raylet)\u001b[0m \u001b[2mInstalled \u001b[1m296 packages\u001b[0m \u001b[2min 347ms\u001b[0m\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=3410205)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(TrainTrainable pid=3410205)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m \u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/mnt/hdd1/dung/protoplast-ml-example/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=3410205)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3410205)\u001b[0m - (node_id=285a3d5c7ed27d7c54d3177df3fb09f94e0b897acd99ca14b9cb0bb5, ip=192.168.1.226, pid=3410711) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m âœ“ Applied AnnDataFileManager patch, AnnData cannot be imported after the patch!\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m =========Starting the training on 0 with num threads: 4=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m   | Name          | Type       | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 0 | loss_fn       | MSELoss    | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 1 | pert_encoder  | Sequential | 1.1 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 2 | basal_encoder | Sequential | 627 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 3 | project_out   | Sequential | 689 K  | train\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m -----------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 1.3 M     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 1.3 M     Total params\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 5.273     Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 16        Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m 0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  6.13it/s]\n",
      "                                                                           \n",
      "Epoch 0:   0%|          | 0/4160 [00:00<?, ?it/s] \n",
      "Epoch 0:   0%|          | 1/4160 [00:52<60:48:48,  0.02it/s, v_num=0]\n",
      "Epoch 0:   0%|          | 2/4160 [00:55<31:51:02,  0.04it/s, v_num=0]\n",
      ".\n",
      ".\n",
      ".\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4158/4160 [19:34<00:00,  3.54it/s, v_num=0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4159/4160 [19:34<00:00,  3.54it/s, v_num=0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [19:34<00:00,  3.54it/s, v_num=0]\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m \n",
      "Validation:   0%|          | 0/1024 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1024 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 1/1024 [00:00<00:17, 59.34it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m \n",
      "Validation DataLoader 0:   0%|          | 2/1024 [00:00<02:39,  6.43it/s]\u001b[A\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m \n",
      ".\n",
      ".\n",
      ".\n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [04:35<00:00,  3.72it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [24:34<00:00,  2.82it/s, v_num=0]       \u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [24:34<00:00,  2.82it/s, v_num=0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4160/4160 [24:34<00:00,  2.82it/s, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/TorchTrainer_2025-09-29_17-14-19/TorchTrainer_bc5c2_00000_0_2025-09-29_17-14-19/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "\u001b[36m(RayTrainWorker pid=3410711)\u001b[0m [rank0]:[W929 17:40:51.121165504 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EmbedSumPerturbationModel_trainer = RayTrainRunner(\n",
    "    EmbedSumPerturbationModel,\n",
    "    EmbedSumAnnDataset,\n",
    "    [\n",
    "        \"input_dim\",\n",
    "        \"output_dim\",\n",
    "        \"hidden_dim\",\n",
    "        \"pert_dim\",\n",
    "        \"lr\",\n",
    "        \"control_pert\",  # \"DMSO_TF\"\n",
    "        \"embed_key\",\n",
    "        \"output_space\",  # \"gene\"\n",
    "    ],\n",
    "    embedsum_metadata_cb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff4544",
   "metadata": {},
   "source": [
    "On a machine with **1 GPU (NVIDIA GeForce RTX 3080 - 12 GiB)**, **96 CPUs**, and **125 GiB RAM**, running `EmbedSumPerturbationModel_trainer.train()` completed in approximately **27 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Pvdt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting thread_per_worker to half of the available CPUs capped at 4\n",
      "Using 1 workers with {'CPU': 4} each\n",
      "=========Length of val_split 65 length of test_split 0 length of train_split 262\n",
      "=========Length of after dropping remainder val_split 64 length of test_split 0 length of train_split 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:14:19,631\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting time: 24.40 seconds\n",
      "Spawning Ray worker and initiating distributed training\n",
      "== Status ==\n",
      "Current time: 2025-09-29 17:14:19 (running for 00:00:00.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/96 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_17-13-43_388460_3349722/artifacts/2025-09-29_17-14-19/TorchTrainer_2025-09-29_17-14-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-29 17:14:55 (running for 00:00:35.42)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_17-13-43_388460_3349722/artifacts/2025-09-29_17-14-19/TorchTrainer_2025-09-29_17-14-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-09-29 17:40:49 (running for 00:26:30.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_17-13-43_388460_3349722/artifacts/2025-09-29_17-14-19/TorchTrainer_2025-09-29_17-14-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 17:40:50,117\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/dtran/protoplast_results/TorchTrainer_2025-09-29_17-14-19' in 0.0069s.\n",
      "2025-09-29 17:40:50,123\tINFO tune.py:1041 -- Total run time: 1590.49 seconds (1590.47 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-09-29 17:40:50 (running for 00:26:30.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/96 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-09-29_17-13-43_388460_3349722/artifacts/2025-09-29_17-14-19/TorchTrainer_2025-09-29_17-14-19/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "CPU times: user 45.4 s, sys: 9.44 s, total: 54.8 s\n",
      "Wall time: 26min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'train_loss': 0.51564621925354, 'val_loss': 0.6005661487579346, 'epoch': 0, 'step': 4160},\n",
       "  path='/home/dtran/protoplast_results/TorchTrainer_2025-09-29_17-14-19/TorchTrainer_bc5c2_00000_0_2025-09-29_17-14-19',\n",
       "  filesystem='local',\n",
       "  checkpoint=Checkpoint(filesystem=local, path=/home/dtran/protoplast_results/TorchTrainer_2025-09-29_17-14-19/TorchTrainer_bc5c2_00000_0_2025-09-29_17-14-19/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "EmbedSumPerturbationModel_trainer.train(\n",
    "    file_paths,\n",
    "    batch_size,  # 2000\n",
    "    test_size,  # 0.0\n",
    "    val_size,  # 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3701b7c6-d275-42a9-9819-8ed73b04d218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
